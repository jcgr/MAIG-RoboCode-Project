\section{Methods}
\label{04}

\subsection{Monte-Carlo implementation}
\label{04_MCTS}

Our implementation of the MCTS algorithm differs from the original MCTS algorithms in a couple of ways. One is one we read about in relation to using MCTS for Pac-Man\cite{pepels2012enhancements}, the other is specific to Robocode and how our simulation works.

\subsubsection{Child selection}

When a node selects which child it should explore, normal MCTS chooses based on the UCT\cite{kocsis2006bandit} value of the children. It can happen, however, that a child ends up in a loss on the first exploration how the simulation plays out, even though most ways through that child are good. This leaves that child with a low UCT value, and it is thus not explored again for a long time. In order to prevent this from happening, we only use UCT for exploration once all children have been visited more than 3 times.

\subsubsection{Max search depth}

Due to the nature of Robocode, we are unable to simulate our opponents properly. This means that our simulations become more inaccurate the further down the tree we go. We chose to incorporate a maximum search depth to prevent simulations that were too inaccurate from disturbing the tree too much. We chose a maximum search depth of 25, as it lets us reach a simulation that is relevant, without risking that the simulation is almost the opposite of what is actually happening.




% Cut after time
% Child exploration
% Max depth during search

How does your algorithm work? Describe in as much detail as you can fit into the report. 
%Differences between standard MCTS and ours (see pacman paper?)
%Monte-Carlo Tree Search parameters (both static and variable)
%Gamestate representation
%Gamestate simulation (incl. assumptions about current gamestate and actions taken by enemy robot)
%Branching factor + interval for considered moves
%Scoring System (both score modifiers and evaluation function)

Also, how did you interface it to the game?
% Part of the game, simple programming task




\subsection{Game Simulation}


\subsection{Evaluation Heuristic}
When the Monte-Carlo Tree Search algorithm calls for the score of a node (game state), we use a heuristic (equation \ref{eq_heuristic_1}) to evaluate the state of the game at that node. 
\begin{equation}
\label{eq_heuristic_1}
score = OurRobotScore +  (\frac{1}{2} \frac{1}{100} EnemyEnergy)
\end{equation}
Equation \ref{eq_heuristic_2} describes how we calculate the score of our own robot.
\\Where \textit{EnergyScore} is $(\frac{1}{100}OurEnergy)$, \textit{BulletDamageScore} is the amount of damage dealt by bullets until this point, \textit{ShootScore} is 0.1 if we shot this round; 0.0 if not. \textit{MovementScore}\footnote{MovementScore is also penalized if the robot moves too close to the walls of the battlefield.} is the change in velocity that the robot made. If the robot did not change velocity or move, \textit{MovementScore} is instead given a $-1$ score. Finally the movement score is multiplied by $1.5$. \textit{RobotHeadingScore} is the amount of degrees that the robot turned this round multiplied by 0.4.
\begin{equation}
\begin{split}
\label{eq_heuristic_2}
ourRobotScore = EnergyScore+BulletDamageScore\\
+ShootScore+MovementScore\\
+RobotHeadingScore
\end{split}
\end{equation}
